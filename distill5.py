# distill5.py - è¶…ç´šçµ‚æ¥µçªç ´ç­–ç•¥ - æ¥µé™ç‰ˆ
from ultralytics import YOLO
from ultralytics.utils import LOGGER
import torch.nn as nn
import torch

# ä½¿ç”¨æœ€ä½³FGDæ¨¡å‹ä½œç‚ºèµ·é»
student_model = YOLO("distill_projects/yolo11n_fgd_breakthrough/weights/best.pt")
teacher_model = YOLO("yolo11x-pose.pt")  # æœ€å¼·æ•™å¸«æ¨¡å‹

# è‡ªå®šç¾©æ›´å¼·å¤§çš„FGDè’¸é¤¾æå¤± - ç›´æ¥æ’å…¥åˆ°å·¥ä½œæµç¨‹ä¸­
class EnhancedFGDLoss(nn.Module):
    """æ¥µé™å¢å¼·ç‰ˆç‰¹å¾µå¼•å°è’¸é¤¾ - å°ˆç‚ºçªç ´æ€§èƒ½ç“¶é ¸å„ªåŒ–"""
    def __init__(self, student_channels, teacher_channels, spatial_weight=3.0, channel_weight=0.4):
        super(EnhancedFGDLoss, self).__init__()
        self.spatial_weight = spatial_weight    # æ›´å¼·çš„ç©ºé–“æ¬Šé‡
        self.channel_weight = channel_weight    # é™ä½é€šé“æ¬Šé‡
        
        # å¢å¼·ç‰ˆé‚Šç·£æ„ŸçŸ¥æ©Ÿåˆ¶
        self.edge_filter = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False).to(
            'cuda' if torch.cuda.is_available() else 'cpu')
        self.edge_filter.weight.data.copy_(torch.tensor([
            [-1, -1, -1],
            [-1,  8, -1],
            [-1, -1, -1]
        ]).view(1, 1, 3, 3) / 8.0)
        self.edge_filter.requires_grad_(False)  # å‡çµåƒæ•¸
        
        # é—œéµé»æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.keypoint_attention = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(8, 1, kernel_size=3, padding=1),
            nn.Sigmoid()
        ).to('cuda' if torch.cuda.is_available() else 'cpu')
        
    def forward(self, y_s, y_t):
        """è¨ˆç®—æ¥µé™å¢å¼·ç‰ˆFGDæå¤±ï¼Œç‰¹åˆ¥å„ªåŒ–å§¿æ…‹ä¼°è¨ˆçš„ç©ºé–“ç‰¹å¾µä¿ç•™"""
        assert len(y_s) == len(y_t)
        losses = []
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            if s.size(2) != t.size(2) or s.size(3) != t.size(3):
                t = torch.nn.functional.interpolate(t, size=(s.size(2), s.size(3)), mode='bilinear', align_corners=False)
                
            b, c, h, w = s.shape
            
            # 1. è‡ªé©æ‡‰ç‰¹å¾µåŒ¹é… - ä½¿ç”¨Huberæå¤±æ¸›å°‘ç•°å¸¸å€¼å½±éŸ¿
            # å°é‡è¦ç‰¹å¾µè³¦äºˆæ›´é«˜æ¬Šé‡
            feature_importance = torch.sigmoid(torch.sum(t, dim=1, keepdim=True))
            l1_loss = torch.nn.functional.smooth_l1_loss(s * feature_importance, t * feature_importance)
            
            # 2. è¶…ç´šå¢å¼·ç‰ˆç©ºé–“æ³¨æ„åŠ›æå¤±
            s_spatial = torch.mean(s, dim=1, keepdim=True)  # [b, 1, h, w]
            t_spatial = torch.mean(t, dim=1, keepdim=True)  # [b, 1, h, w]
            
            # æå–ç©ºé–“ç‰¹å¾µçš„é‚Šç·£ä¿¡æ¯
            s_edge = self.edge_filter(s_spatial)
            t_edge = self.edge_filter(t_spatial)
            
            # é—œéµé»æ³¨æ„åŠ›å¢å¼·
            keypoint_attn = self.keypoint_attention(t_spatial)
            
            # ç©ºé–“æ³¨æ„åŠ›+é‚Šç·£æ„ŸçŸ¥+é—œéµé»æ³¨æ„åŠ›çš„çµ„åˆæå¤±
            spatial_loss = (torch.nn.functional.mse_loss(s_spatial, t_spatial) * 1.5 + 
                         torch.nn.functional.mse_loss(s_edge, t_edge) * 1.0 +
                         torch.nn.functional.mse_loss(s_spatial * keypoint_attn, t_spatial * keypoint_attn) * 1.5) * self.spatial_weight
            
            # 3. å„ªåŒ–çš„é€šé“é—œä¿‚æå¤± - å°ˆæ³¨æ–¼é‡è¦é€šé“
            s_flat = s.view(b, c, -1)  # [b, c, h*w]
            t_flat = t.view(b, c, -1)  # [b, c, h*w]
            
            # é€šé“æ­¸ä¸€åŒ–
            s_flat_norm = torch.nn.functional.normalize(s_flat, dim=2)
            t_flat_norm = torch.nn.functional.normalize(t_flat, dim=2)
            
            # è¨ˆç®—é€šé“ç›¸é—œçŸ©é™£
            s_corr = torch.bmm(s_flat_norm, s_flat_norm.transpose(1, 2)) / (h*w)  # [b, c, c]
            t_corr = torch.bmm(t_flat_norm, t_flat_norm.transpose(1, 2)) / (h*w)  # [b, c, c]
            
            # é€šé“ç›¸é—œæ€§æå¤±
            channel_loss = torch.nn.functional.mse_loss(s_corr, t_corr) * self.channel_weight
            
            # 4. çµ„åˆæå¤±
            total_loss = l1_loss + spatial_loss + channel_loss
            losses.append(total_loss)
            
        loss = sum(losses)
        return loss

# åˆå§‹åŒ–å¢å¼·ç‰ˆFGDæå¤±å‡½æ•¸
try:
    LOGGER.info("åˆå§‹åŒ–å¢å¼·ç‰ˆFGDæå¤±å‡½æ•¸...")
    enhanced_fgd = EnhancedFGDLoss(
        student_channels=[256, 512, 512], 
        teacher_channels=[256, 512, 512]
    )
    # å°‡è‡ªå®šç¾©æå¤±å‡½æ•¸æ³¨å…¥utils.distillæ¨¡å¡Š
    import ultralytics.utils.distill
    ultralytics.utils.distill.FGDLoss = EnhancedFGDLoss
    LOGGER.info("æˆåŠŸæ³¨å…¥å¢å¼·ç‰ˆFGDæå¤±å‡½æ•¸ï¼")
except Exception as e:
    LOGGER.warning(f"æ³¨å…¥å¢å¼·FGDæå¤±å‡½æ•¸å¤±æ•—: {e}")

# æ¥µé™ç‰ˆè‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´å›èª¿
def extreme_adaptive_lr_callback(trainer):
    """æ¥µé™ç‰ˆè‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ï¼Œç”¨æ–¼é¡¯è‘—çªç ´æ€§èƒ½ç“¶é ¸"""
    # åœ¨è¨“ç·´çš„ä¸åŒéšæ®µå¯¦æ–½æ›´æ¿€é€²ç­–ç•¥
    if trainer.epoch == 0:
        # åˆå§‹éšæ®µä½¿ç”¨è¼ƒé«˜å­¸ç¿’ç‡
        LOGGER.info("Initial phase: using higher learning rate")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.00005  # é–‹å§‹éšæ®µæ›´é«˜
    
    elif trainer.epoch == 3:
        # ç¬¬ä¸€æ¬¡å¤§å¹…æé«˜å­¸ç¿’ç‡çªç ´å¹³å°
        LOGGER.info("First major learning rate boost to escape initial plateau")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.0001  # ç¬¬ä¸€æ¬¡å¤§å¹…æé«˜
            
    elif trainer.epoch == 5:
        # ç¬¬äºŒæ¬¡æé«˜å­¸ç¿’ç‡çªç ´ä¸‹ä¸€å€‹å¹³å°
        LOGGER.info("Second learning rate boost for breakthrough")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.00012  # ç¬¬äºŒæ¬¡å¤§å¹…æé«˜
            
    elif trainer.epoch == 8:
        # æ¢å¾©åˆ°ä¸­ç­‰å­¸ç¿’ç‡é€²è¡Œå„ªåŒ–
        LOGGER.info("Restoring to medium learning rate for optimization")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.00002  # ä¸­ç­‰å­¸ç¿’ç‡ç²¾ç´°å„ªåŒ–
    
    elif trainer.epoch == 15:
        # é™ä½å­¸ç¿’ç‡é€²è¡Œç²¾ç´°å„ªåŒ–éšæ®µ
        LOGGER.info("Fine-tuning phase: low learning rate")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.000005  # ç²¾ç´°å„ªåŒ–å­¸ç¿’ç‡
            
    elif trainer.epoch == 20:
        # æ¥µé™ç²¾ç´°å„ªåŒ–éšæ®µ
        LOGGER.info("Ultra-fine tuning phase: minimal learning rate")
        for g in trainer.optimizer.param_groups:
            g['lr'] = 0.000001  # æ¥µé™ç²¾ç´°å„ªåŒ–å­¸ç¿’ç‡
            
    # é¡¯ç¤ºç•¶å‰å­¸ç¿’ç‡
    for i, g in enumerate(trainer.optimizer.param_groups):
        LOGGER.info(f"Epoch {trainer.epoch}: Group {i} learning rate = {g['lr']:.8f}")

# å°‡æ¥µé™ç‰ˆå›èª¿æ·»åŠ åˆ°æ¨¡å‹ä¸­
student_model.add_callback("on_train_epoch_start", extreme_adaptive_lr_callback)

# å¢å¼·çš„ç›£æ§è¨“ç·´é€²åº¦å›èª¿ - ç¶­æŒä¸è®Šï¼Œç›®æ¨™è¨­å®šå·²ç¶“è¶³å¤ ç©æ¥µ
def training_progress_callback(trainer):
    """å¢å¼·ç‰ˆè¨“ç·´é€²åº¦ç›£æ§ï¼Œè¨­å®šæ›´é«˜çš„ç›®æ¨™"""
    if not hasattr(trainer, 'metrics') or trainer.metrics is None:
        return
    
    metrics = trainer.metrics
    if "pose_map50-95" in metrics:
        current_map = metrics.get("pose_map50-95", 0)
        LOGGER.info(f"Epoch {trainer.epoch}: Pose mAP50-95 = {current_map:.6f}")
        
        # çªç ´å®˜æ–¹è¨˜éŒ„æç¤ºï¼Œè¨­å®šæ›´é«˜ç›®æ¨™
        if current_map > 0.505:
            LOGGER.info(f"ğŸš€ çªç ´å®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.505")
            
        if current_map > 0.510:
            LOGGER.info(f"ğŸ”¥ æ˜é¡¯è¶…è¶Šå®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.510")
            
        if current_map > 0.515:
            LOGGER.info(f"ğŸ’¯ å¤§å¹…è¶…è¶Šå®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.515")
            
        if current_map > 0.520:
            LOGGER.info(f"ğŸ† æ¥µé™çªç ´ï¼ç•¶å‰mAP: {current_map:.6f} > 0.520")

# æ·»åŠ ç›£æ§å›èª¿
student_model.add_callback("on_fit_epoch_end", training_progress_callback)

# é«˜ç´šé¸æ“‡æ€§å¢å¼·å›èª¿ - æ ¹æ“šè¨“ç·´éšæ®µéˆæ´»èª¿æ•´æ•¸æ“šå¢å¼·ç­–ç•¥
def advanced_augmentation_callback(trainer):
    """æ ¹æ“šè¨“ç·´éšæ®µé«˜åº¦éˆæ´»åœ°èª¿æ•´å¢å¼·ç­–ç•¥"""
    if trainer.epoch == 0:
        # åˆå§‹éšæ®µï¼šä½¿ç”¨è¼•å¾®å¢å¼·
        LOGGER.info("Initial phase: very light augmentation")
        trainer.train_loader.dataset.hsv_values = [0.05, 0.05, 0.05]  # æ¥µè¼•å¾®HSVè®ŠåŒ–
        if hasattr(trainer.train_loader.dataset, 'mosaic'):
            trainer.train_loader.dataset.mosaic = False   # åˆå§‹å°±ç¦ç”¨é¦¬è³½å…‹
        if hasattr(trainer.train_loader.dataset, 'mixup'):
            trainer.train_loader.dataset.mixup = False    # ç¦ç”¨mixup
            
    elif trainer.epoch == 5:
        # ä¸­æœŸéšæ®µï¼šå¢åŠ è®ŠåŒ–ä»¥å¹«åŠ©æ³›åŒ–
        LOGGER.info("Mid phase: moderate augmentation for diversity")
        trainer.train_loader.dataset.hsv_values = [0.15, 0.15, 0.1]  # é©ä¸­HSVè®ŠåŒ–
        if hasattr(trainer.train_loader.dataset, 'translate'):
            trainer.train_loader.dataset.translate = 0.1  # è¼•å¾®å¹³ç§»
        
    elif trainer.epoch == 10:
        # ä¸­å¾ŒæœŸï¼šæ¸›å°‘å¢å¼·ä»¥æé«˜ç²¾åº¦
        LOGGER.info("Late mid phase: reducing augmentation for precision")
        trainer.train_loader.dataset.hsv_values = [0.1, 0.1, 0.05]  # é™ä½HSVè®ŠåŒ–
        if hasattr(trainer.train_loader.dataset, 'translate'):
            trainer.train_loader.dataset.translate = 0.0  # ç¦ç”¨å¹³ç§»
            
    elif trainer.epoch == 15:
        # å¾ŒæœŸï¼šç¦ç”¨å¤§éƒ¨åˆ†å¢å¼·å°ˆæ³¨ç²¾ç´°å„ªåŒ–
        LOGGER.info("Late phase: minimal augmentation for fine-tuning")
        trainer.train_loader.dataset.hsv_values = [0.0, 0.0, 0.0]  # ç¦ç”¨HSV
        if hasattr(trainer.train_loader.dataset, 'mosaic'):
            trainer.train_loader.dataset.mosaic = False   # ç¢ºä¿ç¦ç”¨é¦¬è³½å…‹
        if hasattr(trainer.train_loader.dataset, 'mixup'):
            trainer.train_loader.dataset.mixup = False    # ç¢ºä¿ç¦ç”¨mixup
        if hasattr(trainer.train_loader.dataset, 'fliplr'):
            trainer.train_loader.dataset.fliplr = 0.3     # åªä¿ç•™å°‘é‡æ°´å¹³ç¿»è½‰
        
    elif trainer.epoch == 20:
        # æœ€çµ‚éšæ®µï¼šå®Œå…¨ç¦ç”¨æ‰€æœ‰å¢å¼·ä»¥å¯¦ç¾æ¥µè‡´ç²¾åº¦
        LOGGER.info("Final phase: zero augmentation for ultimate precision")
        trainer.train_loader.dataset.hsv_values = [0.0, 0.0, 0.0]  # ç¦ç”¨HSV
        if hasattr(trainer.train_loader.dataset, 'mosaic'):
            trainer.train_loader.dataset.mosaic = False   # ç¢ºä¿ç¦ç”¨é¦¬è³½å…‹
        if hasattr(trainer.train_loader.dataset, 'mixup'):
            trainer.train_loader.dataset.mixup = False    # ç¢ºä¿ç¦ç”¨mixup
        if hasattr(trainer.train_loader.dataset, 'fliplr'):
            trainer.train_loader.dataset.fliplr = 0.0     # ç¦ç”¨æ°´å¹³ç¿»è½‰
        
# æ·»åŠ é«˜ç´šé¸æ“‡æ€§å¢å¼·å›èª¿
student_model.add_callback("on_train_epoch_start", advanced_augmentation_callback)

# è‡ªå®šç¾©å„ªåŒ–å™¨è¨­ç½®å›èª¿ - æ§åˆ¶å‹•é‡å’Œbetaå€¼
def optimizer_config_callback(trainer):
    """å„ªåŒ–å„ªåŒ–å™¨åƒæ•¸ä»¥å¯¦ç¾æ›´å¥½çš„æ”¶æ–‚å’Œçªç ´"""
    # åªåœ¨ç¬¬ä¸€å€‹epochè¨­ç½®
    if trainer.epoch == 0:
        for g in trainer.optimizer.param_groups:
            # å°æ–¼AdamWå„ªåŒ–å™¨ï¼Œè¨­ç½®betaå€¼
            if 'betas' in g:
                g['betas'] = (0.937, 0.999)  # å„ªåŒ–çš„betaå€¼
                LOGGER.info(f"è¨­ç½®å„ªåŒ–çš„betaå€¼: {g['betas']}")
            
            # è¨­ç½®å‹•é‡å€¼ï¼ˆå¦‚æœé©ç”¨ï¼‰
            if 'momentum' in g:
                g['momentum'] = 0.95  # å„ªåŒ–çš„å‹•é‡å€¼
                LOGGER.info(f"è¨­ç½®å„ªåŒ–çš„å‹•é‡å€¼: {g['momentum']}")

# æ·»åŠ å„ªåŒ–å™¨è¨­ç½®å›èª¿
student_model.add_callback("on_train_epoch_start", optimizer_config_callback)

# è¶…æ¥µé™çµ‚æ¥µè¨“ç·´è¨­ç½®
student_model.train(
    data="coco-pose.yaml",
    teacher=teacher_model.model,
    distillation_loss="fgd",           # ä½¿ç”¨æˆ‘å€‘å¢å¼·ç‰ˆçš„FGD
    distillation_layers=["4", "6", "8", "11", "13", "16", "19", "22"],  # æ›´å…¨é¢çš„å±¤é¸æ“‡
    epochs=30,                         # å¤§å¹…å»¶é•·è¨“ç·´æ™‚é–“
    batch=16,                          # è¼ƒå°æ‰¹æ¬¡æé«˜ç²¾åº¦
    workers=8,
    lr0=0.00005,                       # æ›´é«˜åˆå§‹å­¸ç¿’ç‡
    lrf=0.00001,                       # æ›´æ¿€é€²çš„è¡°æ¸›
    optimizer="AdamW",                 # æ›´å…ˆé€²çš„å„ªåŒ–å™¨
    weight_decay=0.0001,               # é©åº¦å¢åŠ æ­£å‰‡åŒ–
    cos_lr=False,                      # ç¦ç”¨é¤˜å¼¦èª¿åº¦ï¼Œä½¿ç”¨è‡ªå®šç¾©èª¿åº¦
    patience=35,                       # å¢åŠ è€å¿ƒå€¼
    device=0,
    project="distill_projects",
    name="yolo11n_extreme_breakthrough",
    val=True,
    save_period=1,
    plots=True,
    distill=0.45,                      # é€²ä¸€æ­¥å¢å¼·è’¸é¤¾æ¬Šé‡
    pose=45.0,                         # é€²ä¸€æ­¥å¢å¼·å§¿æ…‹æ¬Šé‡
    warmup_epochs=0.0,                 # ç„¡éœ€ç†±èº«
    close_mosaic=0,                    # å¾é ­é—œé–‰mosaic
    amp=False,                         # ç¢ºä¿å…¨ç²¾åº¦
    overlap_mask=True,                 # æ”¹å–„é®æ“‹æƒ…æ³ (ä¿®æ­£overlapâ†’overlap_mask)
    
    # é€²éšæ•¸æ“šè™•ç†è¨­ç½®
    hsv_h=0.05,                        # åˆå§‹æ¥µè¼•å¾®è‰²èª¿å¢å¼·
    hsv_s=0.05,                        # åˆå§‹æ¥µè¼•å¾®é£½å’Œåº¦å¢å¼·
    hsv_v=0.05,                        # åˆå§‹æ¥µè¼•å¾®äº®åº¦å¢å¼·
    degrees=0.0,                       # ç¦ç”¨æ—‹è½‰å¢å¼·
    translate=0.0,                     # åˆå§‹ç¦ç”¨å¹³ç§»å¢å¼·
    scale=0.0,                         # ç¦ç”¨ç¸®æ”¾å¢å¼·
    fliplr=0.5,                        # å•Ÿç”¨æ°´å¹³ç¿»è½‰ä»¥å¢åŠ å¤šæ¨£æ€§
    mosaic=0.0,                        # ç¦ç”¨é¦¬è³½å…‹å¢å¼·
    
    # ç‰¹æ®Šé—œéµé»è¨­ç½® (ç§»é™¤ç„¡æ•ˆçš„kpt_shapeåƒæ•¸)
    single_cls=False,                  # ç¶­æŒå¤šé¡åˆ¥å€åˆ†
    nbs=8,                             # æ¨™ç¨±æ‰¹æ¬¡å¤§å°
    rect=False,                        # éçŸ©å½¢è¨“ç·´
    save_json=True,                    # ä¿å­˜è©•ä¼°JSON
    half=False,                        # ç¢ºä¿å…¨ç²¾åº¦è¨“ç·´
    augment=False,                     # åˆå§‹ç¦ç”¨å¼·å¢å¼·
    fraction=1.0,                      # ä½¿ç”¨å…¨éƒ¨æ•¸æ“š
    cache="disk",                      # ä½¿ç”¨ç£ç›¤ç·©å­˜åŠ é€Ÿ
    
    # é¡å¤–å„ªåŒ–åƒæ•¸
    dropout=0.0,                       # ç¦ç”¨dropoutä»¥ä¿ç•™å…¨éƒ¨ç‰¹å¾µ
    verbose=True,                      # è©³ç´°æ—¥èªŒ
    seed=42,                           # æ›´å„ªçš„éš¨æ©Ÿç¨®å­
) 