# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import math
import random
from copy import copy

import torch.nn as nn
import torch
import torch.optim as optim

from ultralytics.models import yolo
from ultralytics.nn.tasks import PoseModel
from ultralytics.utils import DEFAULT_CFG, LOGGER, callbacks
from ultralytics.utils.distill import DistillationLoss
from ultralytics.utils.plotting import plot_images, plot_results


class PoseTrainer(yolo.detect.DetectionTrainer):
    """
    A class extending the DetectionTrainer class for training YOLO pose estimation models.

    This trainer specializes in handling pose estimation tasks, managing model training, validation, and visualization
    of pose keypoints alongside bounding boxes.

    Attributes:
        args (dict): Configuration arguments for training.
        model (PoseModel): The pose estimation model being trained.
        data (dict): Dataset configuration including keypoint shape information.
        loss_names (Tuple[str]): Names of the loss components used in training.

    Methods:
        get_model: Retrieves a pose estimation model with specified configuration.
        set_model_attributes: Sets keypoints shape attribute on the model.
        get_validator: Creates a validator instance for model evaluation.
        plot_training_samples: Visualizes training samples with keypoints.
        plot_metrics: Generates and saves training/validation metric plots.

    Examples:
        >>> from ultralytics.models.yolo.pose import PoseTrainer
        >>> args = dict(model="yolo11n-pose.pt", data="coco8-pose.yaml", epochs=3)
        >>> trainer = PoseTrainer(overrides=args)
        >>> trainer.train()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initialize a PoseTrainer object for training YOLO pose estimation models.

        This initializes a trainer specialized for pose estimation tasks, setting the task to 'pose' and
        handling specific configurations needed for keypoint detection models.

        Args:
            cfg (dict, optional): Default configuration dictionary containing training parameters.
            overrides (dict, optional): Dictionary of parameter overrides for the default configuration.
            _callbacks (list, optional): List of callback functions to be executed during training.

        Notes:
            This trainer will automatically set the task to 'pose' regardless of what is provided in overrides.
            A warning is issued when using Apple MPS device due to known bugs with pose models.

        Examples:
            >>> from ultralytics.models.yolo.pose import PoseTrainer
            >>> args = dict(model="yolov8n-pose.pt", data="coco8-pose.yaml", epochs=3)
            >>> trainer = PoseTrainer(overrides=args)
            >>> trainer.train()
        """
        if overrides is None:
            overrides = {}
        overrides["task"] = "pose"

        # è¨­ç½®æ•™å¸«æ¨¡å‹å’Œè’¸é¤¾æ–¹æ³•
        self.teacher = overrides.get("teacher", None)
        self.distill_loss_instance = None
        
        # å¦‚æœæœ‰æ•™å¸«æ¨¡å‹å’Œè’¸é¤¾æ–¹æ³•
        if self.teacher is not None:
            # å‰µå»ºè‡ªå®šç¾©å›èª¿åˆ—è¡¨
            if _callbacks is None:
                _callbacks = callbacks.get_default_callbacks()

            _callbacks["on_train_start"].append(self.distill_on_train_start)
            _callbacks["on_train_epoch_start"].append(self.distill_on_epoch_start)
            _callbacks["on_train_epoch_end"].append(self.distill_on_epoch_end)
            _callbacks["on_val_start"].append(self.distill_on_val_start)
            _callbacks["on_val_end"].append(self.distill_on_val_end)
            _callbacks["on_train_end"].append(self.distill_on_train_end)
            _callbacks["teardown"].append(self.distill_teardown)
            
            LOGGER.info("å·²å•Ÿç”¨è’¸é¤¾è¨“ç·´ç­–ç•¥")
        
        # èª¿ç”¨çˆ¶é¡çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__(cfg, overrides, _callbacks)

        # åˆå§‹åŒ–æ•™å¸«æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.teacher is not None:
            # å‡çµæ•™å¸«æ¨¡å‹åƒæ•¸
            for k, v in self.teacher.named_parameters():
                v.requires_grad = False
            self.teacher = self.teacher.to(self.device)
            self.teacher.eval()
            LOGGER.info(f"åˆå§‹åŒ–æ•™å¸«æ¨¡å‹å·²å®Œæˆï¼Œè¨­ç‚ºè©•ä¼°æ¨¡å¼")

        if isinstance(self.args.device, str) and self.args.device.lower() == "mps":
            LOGGER.warning(
                "WARNING âš ï¸ Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. "
                "See https://github.com/ultralytics/ultralytics/issues/4031."
            )


    def compare_bn_statistics(self, teacher_model, student_model, sample_layers=10):
        """æ¯”è¼ƒæ•™å¸«å’Œå­¸ç”Ÿæ¨¡å‹çš„BNå±¤çµ±è¨ˆæ•¸æ“š"""
        teacher_bns = [m for m in teacher_model.modules() if isinstance(m, nn.BatchNorm2d)]
        student_bns = [m for m in student_model.modules() if isinstance(m, nn.BatchNorm2d)]
        
        # è¨˜éŒ„å…©å€‹æ¨¡å‹çš„BNå±¤æ•¸é‡
        LOGGER.info(f"BNå±¤æ•¸é‡: æ•™å¸«({len(teacher_bns)}), å­¸ç”Ÿ({len(student_bns)})")
        
        # å‰µå»ºå…©å€‹åˆ—è¡¨å­˜å„²BNå±¤çš„å¤§å°ä¿¡æ¯
        teacher_bn_sizes = [bn.num_features for bn in teacher_bns]
        student_bn_sizes = [bn.num_features for bn in student_bns]
        
        # åªæ¯”è¼ƒå¤§å°ç›¸åŒçš„BNå±¤
        comparable_bns = []
        for i, (t_bn, s_bn) in enumerate(zip(teacher_bns, student_bns)):
            if t_bn.num_features == s_bn.num_features:
                comparable_bns.append((i, t_bn, s_bn))
        
        LOGGER.info(f"å¯æ¯”è¼ƒçš„BNå±¤æ•¸é‡: {len(comparable_bns)}/{min(len(teacher_bns), len(student_bns))}")
        
        if not comparable_bns:
            LOGGER.warning("æ²’æœ‰å¯æ¯”è¼ƒçš„BNå±¤ï¼Œè·³éçµ±è¨ˆæ•¸æ“šæ¯”è¼ƒ")
            return {
                'avg_mean_diff': 0,
                'avg_var_diff': 0,
                'avg_mean_rel_diff': 0,
                'avg_var_rel_diff': 0,
                'max_mean_diff': 0,
                'max_var_diff': 0,
                'max_mean_rel_diff': 0, 
                'max_var_rel_diff': 0,
                'mean_diffs': [],
                'var_diffs': []
            }
        
        # è¨ˆç®—æ‰€æœ‰å¯æ¯”è¼ƒBNå±¤çµ±è¨ˆæ•¸æ“šçš„å·®ç•°
        mean_diffs = []
        var_diffs = []
        mean_rel_diffs = []  # ç›¸å°å·®ç•°
        var_rel_diffs = []   # ç›¸å°å·®ç•°
        
        for idx, t_bn, s_bn in comparable_bns:
            # è¨ˆç®—çµ•å°å·®ç•°
            mean_diff = (t_bn.running_mean - s_bn.running_mean).abs().mean().item()
            var_diff = (t_bn.running_var - s_bn.running_var).abs().mean().item()
            
            # è¨ˆç®—ç›¸å°å·®ç•° (é¿å…é™¤ä»¥é›¶)
            t_mean_abs = t_bn.running_mean.abs().mean().item()
            t_var_abs = t_bn.running_var.abs().mean().item()
            
            mean_rel_diff = mean_diff / (t_mean_abs + 1e-6)
            var_rel_diff = var_diff / (t_var_abs + 1e-6)
            
            mean_diffs.append(mean_diff)
            var_diffs.append(var_diff)
            mean_rel_diffs.append(mean_rel_diff)
            var_rel_diffs.append(var_rel_diff)
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆæ•¸æ“š
        avg_mean_diff = sum(mean_diffs) / len(mean_diffs) if mean_diffs else 0
        avg_var_diff = sum(var_diffs) / len(var_diffs) if var_diffs else 0
        avg_mean_rel_diff = sum(mean_rel_diffs) / len(mean_rel_diffs) if mean_rel_diffs else 0
        avg_var_rel_diff = sum(var_rel_diffs) / len(var_rel_diffs) if var_rel_diffs else 0
        
        max_mean_diff = max(mean_diffs) if mean_diffs else 0
        max_var_diff = max(var_diffs) if var_diffs else 0
        max_mean_rel_diff = max(mean_rel_diffs) if mean_rel_diffs else 0
        max_var_rel_diff = max(var_rel_diffs) if var_rel_diffs else 0
        
        # æ‰“å°ç¸½é«”çµ±è¨ˆæ•¸æ“š
        LOGGER.info("=" * 50)
        LOGGER.info("BatchNormå±¤çµ±è¨ˆæ•¸æ“šæ¯”è¼ƒ:")
        LOGGER.info(f"ç¸½è¨ˆæ¯”è¼ƒäº† {len(comparable_bns)} å€‹BNå±¤")
        LOGGER.info(f"å¹³å‡çµ•å°å·®ç•° - å‡å€¼: {avg_mean_diff:.6f}, æ–¹å·®: {avg_var_diff:.6f}")
        LOGGER.info(f"å¹³å‡ç›¸å°å·®ç•° - å‡å€¼: {avg_mean_rel_diff:.2%}, æ–¹å·®: {avg_var_rel_diff:.2%}")
        LOGGER.info(f"æœ€å¤§çµ•å°å·®ç•° - å‡å€¼: {max_mean_diff:.6f}, æ–¹å·®: {max_var_diff:.6f}")
        LOGGER.info(f"æœ€å¤§ç›¸å°å·®ç•° - å‡å€¼: {max_mean_rel_diff:.2%}, æ–¹å·®: {max_var_rel_diff:.2%}")
        
        # æ‰“å°éƒ¨åˆ†å±¤çš„è©³ç´°çµ±è¨ˆæ•¸æ“š
        LOGGER.info("-" * 50)
        LOGGER.info("éƒ¨åˆ†BNå±¤çš„è©³ç´°çµ±è¨ˆæ•¸æ“š:")
        
        # é¸æ“‡ä¸€äº›æœ‰ä»£è¡¨æ€§çš„å±¤(å‰å¹¾å±¤ã€ä¸­é–“å¹¾å±¤ã€å¾Œå¹¾å±¤)
        sample_indices = []
        if len(comparable_bns) <= sample_layers:
            sample_indices = list(range(len(comparable_bns)))
        else:
            # é¸æ“‡å‰å¹¾å±¤ã€ä¸­é–“å¹¾å±¤å’Œå¾Œå¹¾å±¤
            front = sample_layers // 3
            middle = sample_layers // 3
            back = sample_layers - front - middle
            
            sample_indices = list(range(front))
            sample_indices += list(range(len(comparable_bns)//2 - middle//2, len(comparable_bns)//2 + middle//2 + middle%2))
            sample_indices += list(range(len(comparable_bns) - back, len(comparable_bns)))
        
        for i in sample_indices:
            if i < len(comparable_bns):
                idx, t_bn, s_bn = comparable_bns[i]
                
                # è¨ˆç®—åŸºæœ¬çµ±è¨ˆæ•¸æ“š
                t_mean = t_bn.running_mean.mean().item()
                t_var = t_bn.running_var.mean().item()
                s_mean = s_bn.running_mean.mean().item()
                s_var = s_bn.running_var.mean().item()
                
                mean_diff = mean_diffs[i]
                var_diff = var_diffs[i]
                mean_rel_diff = mean_rel_diffs[i]
                var_rel_diff = var_rel_diffs[i]
                
                LOGGER.info(f"BNå±¤ {idx} (ç‰¹å¾µæ•¸: {t_bn.num_features}):")
                LOGGER.info(f"  æ•™å¸« - å‡å€¼: {t_mean:.6f}, æ–¹å·®: {t_var:.6f}")
                LOGGER.info(f"  å­¸ç”Ÿ - å‡å€¼: {s_mean:.6f}, æ–¹å·®: {s_var:.6f}")
                LOGGER.info(f"  çµ•å°å·®ç•° - å‡å€¼: {mean_diff:.6f}, æ–¹å·®: {var_diff:.6f}")
                LOGGER.info(f"  ç›¸å°å·®ç•° - å‡å€¼: {mean_rel_diff:.2%}, æ–¹å·®: {var_rel_diff:.2%}")
        
        LOGGER.info("=" * 50)
        
        # è¿”å›å·®ç•°æŒ‡æ¨™ï¼Œå¯ç”¨æ–¼é€²ä¸€æ­¥åˆ†æ
        return {
            'avg_mean_diff': avg_mean_diff,
            'avg_var_diff': avg_var_diff,
            'avg_mean_rel_diff': avg_mean_rel_diff,
            'avg_var_rel_diff': avg_var_rel_diff,
            'max_mean_diff': max_mean_diff,
            'max_var_diff': max_var_diff,
            'max_mean_rel_diff': max_mean_rel_diff,
            'max_var_rel_diff': max_var_rel_diff,
            'mean_diffs': mean_diffs,
            'var_diffs': var_diffs
        }
    
    def verify_initial_model_state(self):
        """åœ¨è¨“ç·´é–‹å§‹å‰é©—è­‰æ•™å¸«å’Œå­¸ç”Ÿæ¨¡å‹çš„åˆå§‹ç‹€æ…‹"""
        LOGGER.info("=" * 50)
        LOGGER.info("é©—è­‰æ•™å¸«å’Œå­¸ç”Ÿæ¨¡å‹çš„åˆå§‹ç‹€æ…‹")
        
        # 1. æ¯”è¼ƒæ¨¡å‹åƒæ•¸
        teacher_params = {name: param.clone() for name, param in self.teacher.named_parameters()}
        student_params = {name: param.clone() for name, param in self.model.named_parameters()}
        
        # æ‰¾å‡ºå…±åŒçš„åƒæ•¸åç¨±
        common_params = set(teacher_params.keys()).intersection(set(student_params.keys()))
        
        # è¨˜éŒ„åƒæ•¸ç¸½æ•¸å’Œå¯æ¯”è¼ƒæ•¸é‡
        total_t_params = len(teacher_params)
        total_s_params = len(student_params)
        common_param_count = len(common_params)
        
        LOGGER.info(f"åƒæ•¸çµ±è¨ˆ: æ•™å¸«({total_t_params}), å­¸ç”Ÿ({total_s_params}), å…±åŒåƒæ•¸({common_param_count})")
        
        # åªæ¯”è¼ƒå¤§å°ç›¸åŒçš„åƒæ•¸
        comparable_params = []
        for name in common_params:
            t_param = teacher_params[name]
            s_param = student_params[name]
            
            if t_param.size() == s_param.size():
                comparable_params.append(name)
        
        LOGGER.info(f"å¯æ¯”è¼ƒçš„åƒæ•¸æ•¸é‡: {len(comparable_params)}/{common_param_count}")
        
        param_diffs = []
        for name in comparable_params:
            diff = (teacher_params[name] - student_params[name]).abs().max().item()
            param_diffs.append((name, diff))
        
        if param_diffs:
            max_param_diff = max(param_diffs, key=lambda x: x[1])
            LOGGER.info(f"åƒæ•¸æœ€å¤§å·®ç•°: {max_param_diff[1]:.10f} (åœ¨å±¤ {max_param_diff[0]})")
        else:
            LOGGER.info("æ²’æœ‰å¯æ¯”è¼ƒçš„åƒæ•¸")
        
        # 2. æ¯”è¼ƒæ¨¡å‹å‰å‘å‚³æ’­ (ä½¿ç”¨ç›¸åŒè¼¸å…¥)
        # æº–å‚™å›ºå®šçš„æ¸¬è©¦è¼¸å…¥
        test_input = torch.rand(1, 3, 640, 640, device=self.device)
        
        # ä¿å­˜æ•™å¸«å’Œå­¸ç”Ÿçš„ç•¶å‰æ¨¡å¼
        teacher_training = self.teacher.training
        student_training = self.model.training
        
        # è¨­ç½®å…©å€‹æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
        self.teacher.eval()
        self.model.eval()
        
        # æ”¶é›†ä¸­é–“ç‰¹å¾µ
        teacher_features = []
        student_features = []
        teacher_hooks = []
        student_hooks = []
        
        # å®šç¾©ç‰¹å¾µæ”¶é›†å‡½æ•¸
        def hook_fn(features_list):
            return lambda module, input, output: features_list.append(output.detach())
        
        # å˜—è©¦ç‚ºç‰¹å®šå±¤è¨»å†Šhooks
        teacher_hook_layers = []
        student_hook_layers = []
        hook_layer_patterns = ["22.cv2"]  # ä½¿ç”¨æ¨¡å¼åŒ¹é…
        
        # å˜—è©¦æŸ¥æ‰¾æ•™å¸«æ¨¡å‹ä¸­çš„å°æ‡‰å±¤
        for name, module in self.teacher.named_modules():
            for pattern in hook_layer_patterns:
                if pattern in name:
                    teacher_hook_layers.append((name, module))
                    LOGGER.info(f"ç‚ºæ•™å¸«æ¨¡å‹è¨»å†Šhook: {name}, è¼¸å‡ºå½¢ç‹€: {getattr(module, 'out_channels', 'unknown')}")
                    teacher_hooks.append(module.register_forward_hook(hook_fn(teacher_features)))
        
        # å˜—è©¦æŸ¥æ‰¾å­¸ç”Ÿæ¨¡å‹ä¸­çš„å°æ‡‰å±¤
        for name, module in self.model.named_modules():
            for pattern in hook_layer_patterns:
                if pattern in name:
                    student_hook_layers.append((name, module))
                    LOGGER.info(f"ç‚ºå­¸ç”Ÿæ¨¡å‹è¨»å†Šhook: {name}, è¼¸å‡ºå½¢ç‹€: {getattr(module, 'out_channels', 'unknown')}")
                    student_hooks.append(module.register_forward_hook(hook_fn(student_features)))
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå±¤ï¼Œè¨˜éŒ„æ‰€æœ‰å¯ç”¨å±¤ä»¥ä¾›åƒè€ƒ
        if not teacher_hook_layers or not student_hook_layers:
            LOGGER.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„å±¤: {hook_layer_patterns}")
            LOGGER.info("æ•™å¸«æ¨¡å‹å¯ç”¨å±¤:")
            for name, _ in self.teacher.named_modules():
                if any(name.endswith(f".cv{i}") for i in range(1, 4)):
                    LOGGER.info(f"  - {name}")
            
            LOGGER.info("å­¸ç”Ÿæ¨¡å‹å¯ç”¨å±¤:")
            for name, _ in self.model.named_modules():
                if any(name.endswith(f".cv{i}") for i in range(1, 4)):
                    LOGGER.info(f"  - {name}")
        
        # åŸ·è¡Œå‰å‘å‚³æ’­
        with torch.no_grad():
            try:
                teacher_output = self.teacher(test_input)
                student_output = self.model(test_input)
                forward_success = True
            except Exception as e:
                LOGGER.error(f"å‰å‘å‚³æ’­å¤±æ•—: {str(e)}")
                forward_success = False
        
        # ç§»é™¤hooks
        for hook in teacher_hooks + student_hooks:
            hook.remove()
        
        if forward_success:
            # æ¯”è¼ƒæœ€çµ‚è¼¸å‡º (åªåœ¨è¼¸å‡ºæ˜¯Tensoræˆ–å…·æœ‰ç›¸åŒçµæ§‹æ™‚æ¯”è¼ƒ)
            if (isinstance(teacher_output, torch.Tensor) and isinstance(student_output, torch.Tensor) and 
                teacher_output.size() == student_output.size()):
                output_diff = (teacher_output - student_output).abs().max().item()
                LOGGER.info(f"æœ€çµ‚è¼¸å‡ºæœ€å¤§å·®ç•°: {output_diff:.10f}")
            else:
                LOGGER.info("ç„¡æ³•æ¯”è¼ƒæœ€çµ‚è¼¸å‡º (ä¸åŒçš„è¼¸å‡ºé¡å‹æˆ–å½¢ç‹€)")
                if isinstance(teacher_output, torch.Tensor):
                    LOGGER.info(f"æ•™å¸«è¼¸å‡ºå½¢ç‹€: {teacher_output.shape}")
                if isinstance(student_output, torch.Tensor):
                    LOGGER.info(f"å­¸ç”Ÿè¼¸å‡ºå½¢ç‹€: {student_output.shape}")
            
            # æ¯”è¼ƒä¸­é–“ç‰¹å¾µ (åªæ¯”è¼ƒå¤§å°ç›¸åŒçš„ç‰¹å¾µ)
            comparable_features = []
            for i, (t_feat, s_feat) in enumerate(zip(teacher_features, student_features)):
                if isinstance(t_feat, torch.Tensor) and isinstance(s_feat, torch.Tensor) and t_feat.size() == s_feat.size():
                    comparable_features.append((i, t_feat, s_feat))
            
            LOGGER.info(f"å¯æ¯”è¼ƒçš„ç‰¹å¾µæ•¸é‡: {len(comparable_features)}/{len(teacher_features)}")
            
            for i, t_feat, s_feat in comparable_features:
                feat_diff = (t_feat - s_feat).abs()
                max_diff = feat_diff.max().item()
                mean_diff = feat_diff.mean().item()
                std_diff = feat_diff.std().item()
                
                LOGGER.info(f"å±¤ {i} ç‰¹å¾µå·®ç•°çµ±è¨ˆ (å½¢ç‹€: {t_feat.shape}):")
                LOGGER.info(f"  æœ€å¤§å·®ç•°: {max_diff:.10f}, å¹³å‡å·®ç•°: {mean_diff:.10f}, æ¨™æº–å·®: {std_diff:.10f}")
        
        # æ¢å¾©åŸå§‹æ¨¡å¼
        self.teacher.train(teacher_training)
        self.model.train(student_training)
        
        # 3. ç‰¹æ®Šæª¢æŸ¥ - æ¨¡å‹å…§éƒ¨ç‹€æ…‹
        LOGGER.info("-" * 30)
        LOGGER.info("æª¢æŸ¥æ¨¡å‹å…§éƒ¨ç‹€æ…‹:")
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ç›¸åŒçš„buffer (å¦‚BNå±¤çš„running_mean/var)
        teacher_buffers = {name: buf.clone() for name, buf in self.teacher.named_buffers()}
        student_buffers = {name: buf.clone() for name, buf in self.model.named_buffers()}
        
        # æ‰¾å‡ºå…±åŒçš„ç·©è¡å€åç¨±
        common_buffers = set(teacher_buffers.keys()).intersection(set(student_buffers.keys()))
        
        # åªæ¯”è¼ƒå¤§å°ç›¸åŒçš„ç·©è¡å€
        comparable_buffers = []
        for name in common_buffers:
            if teacher_buffers[name].size() == student_buffers[name].size():
                comparable_buffers.append(name)
        
        LOGGER.info(f"å¯æ¯”è¼ƒçš„buffersæ•¸é‡: {len(comparable_buffers)}/{len(common_buffers)}")
        
        buffer_diffs = []
        for name in comparable_buffers:
            diff = (teacher_buffers[name] - student_buffers[name]).abs().max().item()
            buffer_diffs.append((name, diff))
        
        if buffer_diffs:
            max_buffer_diff = max(buffer_diffs, key=lambda x: x[1])
            LOGGER.info(f"Bufferæœ€å¤§å·®ç•°: {max_buffer_diff[1]:.10f} (åœ¨ {max_buffer_diff[0]})")
        else:
            LOGGER.info("æ²’æœ‰å¯æ¯”è¼ƒçš„buffer")
        
        LOGGER.info("=" * 50)

    def freeze_bn_statistics(self, model):
        """å‡çµæ¨¡å‹ä¸­æ‰€æœ‰BNå±¤çš„çµ±è¨ˆé‡æ›´æ–°"""
        for m in model.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.track_running_stats = False  # åœæ­¢æ›´æ–°running_meanå’Œrunning_var
                m.running_mean.requires_grad = False
                m.running_var.requires_grad = False
        
        LOGGER.info("å·²å‡çµæ¨¡å‹çš„BNçµ±è¨ˆé‡æ›´æ–°")


    def distill_on_train_start(self, trainer):
        # è¨­ç½®ç¢ºå®šæ€§è¨ˆç®—ç’°å¢ƒ
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(42)

    def distill_on_epoch_start(self, trainer):
        # åœ¨æ¯”è¼ƒæ¨¡å‹ç‹€æ…‹å¾Œæ·»åŠ 
        LOGGER.info("æ¯”è¼ƒæ•™å¸«å’Œå­¸ç”Ÿæ¨¡å‹çš„BNå±¤çµ±è¨ˆæ•¸æ“š...")
        bn_diff_stats = self.compare_bn_statistics(self.teacher, self.model)
        
        # æ ¹æ“šå·®ç•°å¤§å°ç™¼å‡ºè­¦å‘Š
        if bn_diff_stats['avg_mean_rel_diff'] > 0.5 or bn_diff_stats['avg_var_rel_diff'] > 0.5:
            LOGGER.warning("è­¦å‘Šï¼šBNå±¤çµ±è¨ˆæ•¸æ“šå·®ç•°é¡¯è‘—ï¼Œå¯èƒ½å½±éŸ¿è’¸é¤¾æ•ˆæœ!")

        self.verify_initial_model_state()

        if self.epoch == 0:
            LOGGER.info("éšæ®µ1: ç´”è’¸é¤¾")

            distillation_loss = "cwd"
            distillation_layers = ["22"]
            self.model.args.distill = 0.1

            # åˆå§‹åŒ–è’¸é¤¾æå¤±å¯¦ä¾‹
            self.distill_loss_instance = DistillationLoss(
                models=self.model,
                modelt=self.teacher,
                distiller=distillation_loss,
                layers=distillation_layers,
            )

            # é¸æ“‡: å‡çµBNçµ±è¨ˆé‡æ›´æ–°
            self.freeze_bn_statistics(self.model)

        # åœ¨è¨“ç·´é–‹å§‹å‰æª¢æŸ¥æ•™å¸«æ¨¡å‹
        LOGGER.info("=" * 50)
        LOGGER.info("æ•™å¸«æ¨¡å‹ç‹€æ…‹æª¢æŸ¥:")
        LOGGER.info(f"æ•™å¸«æ¨¡å‹è©•ä¼°æ¨¡å¼: {'é–‹å•Ÿ' if not self.teacher.training else 'é—œé–‰'}")
        
        teacher_frozen_params = 0
        teacher_trainable_params = 0
        teacher_total_params = 0
        teacher_frozen_layers = []
        teacher_trainable_layers = []
        
        for name, param in self.teacher.named_parameters():
            teacher_total_params += param.numel()
            if param.requires_grad:
                teacher_trainable_params += param.numel()
                teacher_trainable_layers.append(name)
            else:
                teacher_frozen_params += param.numel()
                teacher_frozen_layers.append(name)
                
        LOGGER.info(f"æ•™å¸«æ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
        LOGGER.info(f"ç¸½åƒæ•¸æ•¸é‡: {teacher_total_params:,}")
        LOGGER.info(f"å‡çµåƒæ•¸æ•¸é‡: {teacher_frozen_params:,} ({teacher_frozen_params/teacher_total_params:.2%})")
        LOGGER.info(f"å¯è¨“ç·´åƒæ•¸æ•¸é‡: {teacher_trainable_params:,} ({teacher_trainable_params/teacher_total_params:.2%})")
        LOGGER.info(f"å‡çµå±¤æ•¸: {len(teacher_frozen_layers)}")
        LOGGER.info(f"å¯è¨“ç·´å±¤æ•¸: {len(teacher_trainable_layers)}")
        
        # å¦‚æœæœ‰å¯è¨“ç·´å±¤ï¼Œé€™æ˜¯å€‹è­¦å‘Š
        if teacher_trainable_params > 0:
            LOGGER.warning(f"è­¦å‘Š: æ•™å¸«æ¨¡å‹æœ‰ {len(teacher_trainable_layers)} å€‹å¯è¨“ç·´å±¤!")
            if len(teacher_trainable_layers) > 0:
                LOGGER.warning(f"å¯è¨“ç·´å±¤ç¤ºä¾‹: {teacher_trainable_layers[:5]}")
        
        # æª¢æŸ¥æ•™å¸«æ¨¡å‹çš„BNå±¤ç‹€æ…‹
        teacher_bn_train_count = 0
        teacher_bn_eval_count = 0
        
        for name, module in self.teacher.named_modules():
            if isinstance(module, torch.nn.BatchNorm2d):
                if module.training:
                    teacher_bn_train_count += 1
                else:
                    teacher_bn_eval_count += 1
        
        LOGGER.info(f"æ•™å¸«æ¨¡å‹BNå±¤çµ±è¨ˆ: è¨“ç·´æ¨¡å¼ {teacher_bn_train_count} å€‹, è©•ä¼°æ¨¡å¼ {teacher_bn_eval_count} å€‹")
        if teacher_bn_train_count > 0:
            LOGGER.warning(f"è­¦å‘Š: æ•™å¸«æ¨¡å‹æœ‰ {teacher_bn_train_count} å€‹BNå±¤è™•æ–¼è¨“ç·´æ¨¡å¼!")
        LOGGER.info("=" * 50)

        # æª¢æŸ¥å­¸ç”Ÿæ¨¡å‹åƒæ•¸è¨“ç·´ç‹€æ…‹
        LOGGER.info("å­¸ç”Ÿæ¨¡å‹ç‹€æ…‹æª¢æŸ¥:")
        LOGGER.info(f"å­¸ç”Ÿæ¨¡å‹è©•ä¼°æ¨¡å¼: {'é—œé–‰' if self.model.training else 'é–‹å•Ÿ'}")
        
        total_params = 0
        trainable_params = 0
        frozen_layers = []
        trainable_layers = []

        for name, param in self.model.named_parameters():
            total_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
                trainable_layers.append(name)
            else:
                frozen_layers.append(name)

        LOGGER.info(f"å­¸ç”Ÿæ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
        LOGGER.info(f"ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
        LOGGER.info(f"å¯è¨“ç·´åƒæ•¸æ•¸é‡: {trainable_params:,} ({trainable_params/total_params:.2%})")
        LOGGER.info(f"å‡çµåƒæ•¸æ•¸é‡: {total_params-trainable_params:,} ({(total_params-trainable_params)/total_params:.2%})")
        LOGGER.info(f"å¯è¨“ç·´å±¤æ•¸: {len(trainable_layers)}")
        LOGGER.info(f"å‡çµå±¤æ•¸: {len(frozen_layers)}")

        # æª¢æŸ¥BNå±¤çš„è¨“ç·´ç‹€æ…‹
        bn_train_count = 0
        bn_eval_count = 0
        
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.BatchNorm2d):
                if module.training:
                    bn_train_count += 1
                else:
                    bn_eval_count += 1
        
        LOGGER.info(f"BNå±¤çµ±è¨ˆ: è¨“ç·´æ¨¡å¼ {bn_train_count} å€‹, è©•ä¼°æ¨¡å¼ {bn_eval_count} å€‹")
        
        # å¯ä»¥é¸æ“‡æ€§åœ°åˆ—å°éƒ¨åˆ†è¨“ç·´å±¤å’Œå‡çµå±¤çš„åç¨±ï¼ˆé¿å…è¼¸å‡ºéå¤šï¼‰
        if len(trainable_layers) > 0:
            LOGGER.info(f"è¨“ç·´å±¤ç¤ºä¾‹ (å‰5å€‹): {trainable_layers[:5]}")
        if len(frozen_layers) > 0:
            LOGGER.info(f"å‡çµå±¤ç¤ºä¾‹ (å‰5å€‹): {frozen_layers[:5]}")
        LOGGER.info("=" * 50)

        # æª¢æŸ¥æ¨¡å‹ä¸­çš„éš¨æ©Ÿæ€§ä¾†æº
        LOGGER.info("æª¢æŸ¥æ¨¡å‹ä¸­çš„éš¨æ©Ÿæ€§ä¾†æº:")
        
        # æª¢æŸ¥æ•™å¸«æ¨¡å‹
        teacher_dropout_layers = []
        for name, module in self.teacher.named_modules():
            if isinstance(module, nn.Dropout):
                teacher_dropout_layers.append(name)
        
        if teacher_dropout_layers:
            LOGGER.info(f"æ•™å¸«æ¨¡å‹åŒ…å« {len(teacher_dropout_layers)} å€‹Dropoutå±¤:")
            for name in teacher_dropout_layers[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                LOGGER.info(f"  - {name}")
        else:
            LOGGER.info("æ•™å¸«æ¨¡å‹ä¸åŒ…å«ä»»ä½•Dropoutå±¤")
        
        # æª¢æŸ¥å­¸ç”Ÿæ¨¡å‹çš„éš¨æ©Ÿæ€§ä¾†æº
        student_dropout_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Dropout):
                student_dropout_layers.append(name)
        
        if student_dropout_layers:
            LOGGER.info(f"å­¸ç”Ÿæ¨¡å‹åŒ…å« {len(student_dropout_layers)} å€‹Dropoutå±¤:")
            for name in student_dropout_layers[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                LOGGER.info(f"  - {name}")
        else:
            LOGGER.info("å­¸ç”Ÿæ¨¡å‹ä¸åŒ…å«ä»»ä½•Dropoutå±¤")

        # è¨»å†Šé‰¤å­
        self.distill_loss_instance.register_hook()
        for i, h in enumerate(self.distill_loss_instance.remove_handle):
            LOGGER.info(f"é‰¤å­ {i+1} å·²è¨»å†Š: {h}")
            

    def distill_on_epoch_end(self, trainer):
        self.distill_loss_instance.remove_handle_()
        LOGGER.debug(f"Removed distillation hooks at epoch {self.epoch} end")
        LOGGER.info(f"self.lr: {self.lr}")

    def distill_on_val_start(self, validator):
        self.distill_loss_instance.remove_handle_()
        LOGGER.debug("Ensuring distillation hooks are removed for validation")

    def distill_on_val_end(self, validator):
        pass

    def distill_on_train_end(self, trainer):
        self.distill_loss_instance.remove_handle_()
        LOGGER.info("Cleaned up distillation resources at training end")

    def distill_teardown(self, trainer):
        self.distill_loss_instance.remove_handle_()
        self.distill_loss_instance = None
        LOGGER.debug("Final cleanup of distillation resources during teardown")

    def get_model(self, cfg=None, weights=None, verbose=True):
        """
        Get pose estimation model with specified configuration and weights.

        Args:
            cfg (str | Path | dict | None): Model configuration file path or dictionary.
            weights (str | Path | None): Path to the model weights file.
            verbose (bool): Whether to display model information.

        Returns:
            (PoseModel): Initialized pose estimation model.
        """
        model = PoseModel(cfg, ch=3, nc=self.data["nc"], data_kpt_shape=self.data["kpt_shape"], verbose=verbose)
        if weights:
            model.load(weights)

        return model
    
    def preprocess_batch(self, batch):
        """é è™•ç†æ‰¹æ¬¡æ•¸æ“šä¸¦æ·»åŠ è’¸é¤¾ç›¸é—œä¿¡æ¯"""
        batch = super().preprocess_batch(batch)
        
        # æ·»åŠ è’¸é¤¾ç›¸é—œä¿¡æ¯ï¼Œåƒ…åœ¨å•Ÿç”¨è’¸é¤¾æ™‚æ·»åŠ 
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            batch["distill_instance"] = self.distill_loss_instance
            LOGGER.debug(f"Added distillation instance to batch for epoch {self.epoch}")
        
        return batch

    def set_model_attributes(self):
        """Sets keypoints shape attribute of PoseModel."""
        super().set_model_attributes()
        self.model.kpt_shape = self.data["kpt_shape"]

    def get_validator(self):
        """Returns an instance of the PoseValidator class for validation."""
        self.loss_names = "box_loss", "pose_loss", "kobj_loss", "cls_loss", "dfl_loss", "d_loss"
        return yolo.pose.PoseValidator(
            self.test_loader,
            save_dir=self.save_dir,
            args=copy(self.args),
            _callbacks=self.callbacks
        )

    def plot_training_samples(self, batch, ni):
        """
        Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.

        Args:
            batch (dict): Dictionary containing batch data with the following keys:
                - img (torch.Tensor): Batch of images
                - keypoints (torch.Tensor): Keypoints coordinates for pose estimation
                - cls (torch.Tensor): Class labels
                - bboxes (torch.Tensor): Bounding box coordinates
                - im_file (list): List of image file paths
                - batch_idx (torch.Tensor): Batch indices for each instance
            ni (int): Current training iteration number used for filename

        The function saves the plotted batch as an image in the trainer's save directory with the filename
        'train_batch{ni}.jpg', where ni is the iteration number.
        """
        images = batch["img"]
        kpts = batch["keypoints"]
        cls = batch["cls"].squeeze(-1)
        bboxes = batch["bboxes"]
        paths = batch["im_file"]
        batch_idx = batch["batch_idx"]
        plot_images(
            images,
            batch_idx,
            cls,
            bboxes,
            kpts=kpts,
            paths=paths,
            fname=self.save_dir / f"train_batch{ni}.jpg",
            on_plot=self.on_plot,
        )

    def plot_metrics(self):
        """Plots training/val metrics."""
        plot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png
