# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import math
import random
from copy import copy

import torch.nn as nn
import torch

from ultralytics.models import yolo
from ultralytics.nn.tasks import PoseModel
from ultralytics.utils import DEFAULT_CFG, LOGGER, callbacks
from ultralytics.utils.distill import DistillationLoss
from ultralytics.utils.plotting import plot_images, plot_results


class PoseTrainer(yolo.detect.DetectionTrainer):
    """
    A class extending the DetectionTrainer class for training YOLO pose estimation models.

    This trainer specializes in handling pose estimation tasks, managing model training, validation, and visualization
    of pose keypoints alongside bounding boxes.

    Attributes:
        args (dict): Configuration arguments for training.
        model (PoseModel): The pose estimation model being trained.
        data (dict): Dataset configuration including keypoint shape information.
        loss_names (Tuple[str]): Names of the loss components used in training.

    Methods:
        get_model: Retrieves a pose estimation model with specified configuration.
        set_model_attributes: Sets keypoints shape attribute on the model.
        get_validator: Creates a validator instance for model evaluation.
        plot_training_samples: Visualizes training samples with keypoints.
        plot_metrics: Generates and saves training/validation metric plots.

    Examples:
        >>> from ultralytics.models.yolo.pose import PoseTrainer
        >>> args = dict(model="yolo11n-pose.pt", data="coco8-pose.yaml", epochs=3)
        >>> trainer = PoseTrainer(overrides=args)
        >>> trainer.train()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initialize a PoseTrainer object for training YOLO pose estimation models.

        This initializes a trainer specialized for pose estimation tasks, setting the task to 'pose' and
        handling specific configurations needed for keypoint detection models.

        Args:
            cfg (dict, optional): Default configuration dictionary containing training parameters.
            overrides (dict, optional): Dictionary of parameter overrides for the default configuration.
            _callbacks (list, optional): List of callback functions to be executed during training.

        Notes:
            This trainer will automatically set the task to 'pose' regardless of what is provided in overrides.
            A warning is issued when using Apple MPS device due to known bugs with pose models.

        Examples:
            >>> from ultralytics.models.yolo.pose import PoseTrainer
            >>> args = dict(model="yolov8n-pose.pt", data="coco8-pose.yaml", epochs=3)
            >>> trainer = PoseTrainer(overrides=args)
            >>> trainer.train()
        """
        if overrides is None:
            overrides = {}
        overrides["task"] = "pose"

        # è¨­ç½®æ•™å¸«æ¨¡å‹å’Œè’¸é¤¾æ–¹æ³•
        self.teacher = overrides.get("teacher", None)
        self.distillation_loss = overrides.get("distillation_loss", None)
        self.distillation_layers = overrides.get("distillation_layers", None)
        self.distill_loss_instance = None
        self.pure_distill = overrides.get("pure_distill", False)

        # è’¸é¤¾æ–¹æ³•æ˜ å°„å­—å…¸ï¼Œæ–¹ä¾¿å°‡ç°¡çŸ­åç¨±æ˜ å°„åˆ°å®Œæ•´å¯¦ç¾
        self.distillation_methods = {
            "cwd": "cwd",  # Channel-wise Distillation
            "mgd": "mgd",  # Masked Generative Distillation
            "rev": "rev",  # Review KD
            "fgd": "fgd",  # Feature Guided Distillation
            "pkd": "pkd",  # Probabilistic Knowledge Distillation
            "kd": "cwd",   # é»˜èªä½¿ç”¨ CWD
            "review": "rev",
            "feature": "fgd",
            "enhancedfgd": "enhancedfgd"  # å¢å¼·ç‰ˆç‰¹å¾µå¼•å°è’¸é¤¾
        }
        
        # æ¨™æº–åŒ–è’¸é¤¾æ–¹æ³•åç¨±
        if self.distillation_loss:
            self.distillation_loss = self.distillation_methods.get(
                self.distillation_loss.lower(), self.distillation_loss
            )
        
        # å¦‚æœæœ‰æ•™å¸ˆæ¨¡å‹å’Œè’¸é¦æ–¹æ³•ï¼Œæ·»åŠ å¯¹åº”çš„å›è°ƒ
        if self.teacher is not None and self.distillation_loss is not None:
            # åˆ›å»ºè‡ªå®šä¹‰å›è°ƒåˆ—è¡¨
            if _callbacks is None:
                _callbacks = callbacks.get_default_callbacks()
                
            # æ·»åŠ è’¸é¦ç›¸å…³å›è°ƒåˆ°å„äº‹ä»¶ä¸­
            _callbacks["on_train_start"].append(self.distill_on_train_start)
            _callbacks["on_train_epoch_start"].append(self.distill_on_epoch_start)
            _callbacks["on_train_epoch_end"].append(self.distill_on_epoch_end)
            _callbacks["on_val_start"].append(self.distill_on_val_start)
            _callbacks["on_val_end"].append(self.distill_on_val_end)
            _callbacks["on_train_end"].append(self.distill_on_train_end)
            _callbacks["teardown"].append(self.distill_teardown)
            
            # æ·»åŠ æé™å­¦ä¹ ç‡è°ƒæ•´å’Œé«˜çº§æ•°æ®å¢å¼ºå›è°ƒ
            _callbacks["on_train_epoch_start"].append(self.extreme_adaptive_lr_callback)
            _callbacks["on_train_epoch_start"].append(self.advanced_augmentation_callback)
            _callbacks["on_train_epoch_start"].append(self.optimizer_config_callback)
            
            # æ·»åŠ è®­ç»ƒè¿›åº¦ç›‘æ§å›è°ƒ
            _callbacks["on_fit_epoch_end"].append(self.training_progress_callback)
        
        # èª¿ç”¨çˆ¶é¡çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__(cfg, overrides, _callbacks)

        # åˆå§‹åŒ–æ•™å¸«æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.teacher is not None:
            # å‡çµæ•™å¸«æ¨¡å‹åƒæ•¸
            for k, v in self.teacher.named_parameters():
                v.requires_grad = False
            self.teacher = self.teacher.to(self.device)
            self.teacher.eval()
            LOGGER.info(f"Using {self.distillation_loss} distillation with teacher model")

        if isinstance(self.args.device, str) and self.args.device.lower() == "mps":
            LOGGER.warning(
                "WARNING âš ï¸ Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. "
                "See https://github.com/ultralytics/ultralytics/issues/4031."
            )

    def extreme_adaptive_lr_callback(self, trainer):
        """æ¥µé™ç‰ˆè‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ï¼Œç”¨æ–¼é¡¯è‘—çªç ´æ€§èƒ½ç“¶é ¸"""
        # åœ¨è¨“ç·´çš„ä¸åŒéšæ®µå¯¦æ–½æ›´æ¿€é€²ç­–ç•¥
        LOGGER.info(f"åŸ·è¡Œå­¸ç¿’ç‡èª¿æ•´ - ç•¶å‰ epoch: {trainer.epoch}")
        
        if trainer.epoch == 0:
            # åˆå§‹éšæ®µä½¿ç”¨è¼ƒé«˜å­¸ç¿’ç‡
            target_lr = 0.00007
            LOGGER.info(f"Initial phase: setting higher learning rate to {target_lr}")
        elif trainer.epoch == 3:
            # ç¬¬ä¸€æ¬¡å¤§å¹…æé«˜å­¸ç¿’ç‡çªç ´å¹³å°
            target_lr = 0.0002
            LOGGER.info(f"First major learning rate boost to {target_lr}")
        elif trainer.epoch == 5:
            # ç¬¬äºŒæ¬¡æé«˜å­¸ç¿’ç‡çªç ´ä¸‹ä¸€å€‹å¹³å°
            target_lr = 0.00025
            LOGGER.info(f"Second learning rate boost to {target_lr}")
        elif trainer.epoch == 8:
            # æ¢å¾©åˆ°ä¸­ç­‰å­¸ç¿’ç‡é€²è¡Œå„ªåŒ–
            target_lr = 0.00005
            LOGGER.info(f"Restoring to medium learning rate: {target_lr}")
        elif trainer.epoch == 12:
            # é©åº¦é™ä½å­¸ç¿’ç‡
            target_lr = 0.00002
            LOGGER.info(f"Moderate tuning phase: reduced learning rate to {target_lr}")
        elif trainer.epoch == 15:
            # é™ä½å­¸ç¿’ç‡é€²è¡Œç²¾ç´°å„ªåŒ–éšæ®µ
            target_lr = 0.000005
            LOGGER.info(f"Fine-tuning phase: low learning rate of {target_lr}")
        elif trainer.epoch == 20:
            # æ¥µé™ç²¾ç´°å„ªåŒ–éšæ®µ
            target_lr = 0.000001
            LOGGER.info(f"Ultra-fine tuning phase: minimal learning rate of {target_lr}")
        else:
            # å…¶ä»–epochä¸æ›´æ”¹å­¸ç¿’ç‡ï¼Œä¿æŒåŸå§‹è¨­ç½®
            return
            
        # é¡¯ç¤ºç•¶å‰å­¸ç¿’ç‡
        for i, g in enumerate(trainer.optimizer.param_groups):
            g['lr'] = target_lr
            LOGGER.info(f"Epoch {trainer.epoch}: Group {i} learning rate = {g['lr']:.8f}")
            
    def advanced_augmentation_callback(self, trainer):
        """æ ¹æ“šè¨“ç·´éšæ®µé«˜åº¦éˆæ´»åœ°èª¿æ•´å¢å¼·ç­–ç•¥"""
        if trainer.epoch == 0:
            # åˆå§‹éšæ®µï¼šä½¿ç”¨é©ä¸­å¢å¼·
            LOGGER.info("Initial phase: moderate augmentation")
            if hasattr(trainer.train_loader.dataset, 'hsv_values'):
                trainer.train_loader.dataset.hsv_values = [0.15, 0.15, 0.15]  # é©ä¸­HSVè®ŠåŒ–
            if hasattr(trainer.train_loader.dataset, 'mosaic'):
                trainer.train_loader.dataset.mosaic = False   # åˆå§‹å°±ç¦ç”¨é¦¬è³½å…‹
            if hasattr(trainer.train_loader.dataset, 'mixup'):
                trainer.train_loader.dataset.mixup = False    # ç¦ç”¨mixup
                
        elif trainer.epoch == 3:
            # å¤§å¹…æé«˜å­¸ç¿’ç‡çš„åŒæ™‚å¢åŠ å¢å¼·å¼·åº¦
            LOGGER.info("Boosting phase: stronger augmentation")
            if hasattr(trainer.train_loader.dataset, 'hsv_values'):
                trainer.train_loader.dataset.hsv_values = [0.25, 0.25, 0.2]  # è¼ƒå¼·HSVè®ŠåŒ–
            if hasattr(trainer.train_loader.dataset, 'translate'):
                trainer.train_loader.dataset.translate = 0.15  # è¼•å¾®å¹³ç§»
            if hasattr(trainer.train_loader.dataset, 'scale'):
                trainer.train_loader.dataset.scale = 0.2  # å¢åŠ ç¸®æ”¾
            
        elif trainer.epoch == 8:
            # å­¸ç¿’ç‡ä¸‹é™æ™‚æ¸›å°‘å¢å¼·å¼·åº¦
            LOGGER.info("Mid phase: moderate augmentation")
            if hasattr(trainer.train_loader.dataset, 'hsv_values'):
                trainer.train_loader.dataset.hsv_values = [0.15, 0.15, 0.1]  # é©ä¸­HSVè®ŠåŒ–
            if hasattr(trainer.train_loader.dataset, 'translate'):
                trainer.train_loader.dataset.translate = 0.1  # è¼•å¾®å¹³ç§»
            if hasattr(trainer.train_loader.dataset, 'scale'):
                trainer.train_loader.dataset.scale = 0.1  # æ¸›å°‘ç¸®æ”¾
                
        elif trainer.epoch == 15:
            # å¾ŒæœŸï¼šæ¸›å°‘å¢å¼·å°ˆæ³¨ç²¾ç´°å„ªåŒ–
            LOGGER.info("Late phase: minimal augmentation for fine-tuning")
            if hasattr(trainer.train_loader.dataset, 'hsv_values'):
                trainer.train_loader.dataset.hsv_values = [0.05, 0.05, 0.05]  # è¼•å¾®HSV
            if hasattr(trainer.train_loader.dataset, 'translate'):
                trainer.train_loader.dataset.translate = 0.0  # ç¦ç”¨å¹³ç§»
            if hasattr(trainer.train_loader.dataset, 'scale'):
                trainer.train_loader.dataset.scale = 0.0  # ç¦ç”¨ç¸®æ”¾
            if hasattr(trainer.train_loader.dataset, 'fliplr'):
                trainer.train_loader.dataset.fliplr = 0.3     # åªä¿ç•™å°‘é‡æ°´å¹³ç¿»è½‰
            
        elif trainer.epoch == 20:
            # æœ€çµ‚éšæ®µï¼šå®Œå…¨ç¦ç”¨æ‰€æœ‰å¢å¼·ä»¥å¯¦ç¾æ¥µè‡´ç²¾åº¦
            LOGGER.info("Final phase: zero augmentation for ultimate precision")
            if hasattr(trainer.train_loader.dataset, 'hsv_values'):
                trainer.train_loader.dataset.hsv_values = [0.0, 0.0, 0.0]  # ç¦ç”¨HSV
            if hasattr(trainer.train_loader.dataset, 'mosaic'):
                trainer.train_loader.dataset.mosaic = False   # ç¢ºä¿ç¦ç”¨é¦¬è³½å…‹
            if hasattr(trainer.train_loader.dataset, 'mixup'):
                trainer.train_loader.dataset.mixup = False    # ç¢ºä¿ç¦ç”¨mixup
            if hasattr(trainer.train_loader.dataset, 'fliplr'):
                trainer.train_loader.dataset.fliplr = 0.0     # ç¦ç”¨æ°´å¹³ç¿»è½‰
                
    def training_progress_callback(self, trainer):
        """å¢å¼·ç‰ˆè¨“ç·´é€²åº¦ç›£æ§ï¼Œè¨­å®šæ›´é«˜çš„ç›®æ¨™"""
        if not hasattr(trainer, 'metrics') or trainer.metrics is None:
            return
        
        metrics = trainer.metrics
        if "pose_map50-95" in metrics:
            current_map = metrics.get("pose_map50-95", 0)
            LOGGER.info(f"Epoch {trainer.epoch}: Pose mAP50-95 = {current_map:.6f}")
            
            # çªç ´å®˜æ–¹è¨˜éŒ„æç¤ºï¼Œè¨­å®šæ›´é«˜ç›®æ¨™
            if current_map > 0.505:
                LOGGER.info(f"ğŸš€ çªç ´å®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.505")
                
            if current_map > 0.510:
                LOGGER.info(f"ğŸ”¥ æ˜é¡¯è¶…è¶Šå®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.510")
                
            if current_map > 0.515:
                LOGGER.info(f"ğŸ’¯ å¤§å¹…è¶…è¶Šå®˜æ–¹è¨˜éŒ„ï¼ç•¶å‰mAP: {current_map:.6f} > 0.515")
                
            if current_map > 0.520:
                LOGGER.info(f"ğŸ† æ¥µé™çªç ´ï¼ç•¶å‰mAP: {current_map:.6f} > 0.520")
                
    def optimizer_config_callback(self, trainer):
        """å„ªåŒ–å„ªåŒ–å™¨åƒæ•¸ä»¥å¯¦ç¾æ›´å¥½çš„æ”¶æ–‚å’Œçªç ´"""
        # åªåœ¨ç¬¬ä¸€å€‹epochè¨­ç½®
        if trainer.epoch == 0:
            for g in trainer.optimizer.param_groups:
                # å°æ–¼AdamWå„ªåŒ–å™¨ï¼Œè¨­ç½®betaå€¼
                if 'betas' in g:
                    g['betas'] = (0.937, 0.999)  # å„ªåŒ–çš„betaå€¼

    def distill_on_train_start(self, trainer):
        """è¨“ç·´é–‹å§‹æ™‚åˆå§‹åŒ–è’¸é¤¾æå¤±å¯¦ä¾‹å’Œå‡çµéç›®æ¨™å±¤"""
        if self.teacher is not None and self.distillation_loss is not None:
            # åˆå§‹åŒ–è’¸é¤¾æå¤±å¯¦ä¾‹ï¼Œæ”¯æŒå¢å¼·ç‰ˆFGD
            self.distill_loss_instance = DistillationLoss(
                models=self.model,
                modelt=self.teacher,
                distiller=self.distillation_loss,
                layers=self.distillation_layers
            )

            # åœ¨ç´”è’¸é¤¾æ¨¡å¼ä¸‹é€²è¡Œå„ªåŒ–åƒæ•¸é¸æ“‡
            if self.pure_distill:
                # ç²å–éœ€è¦è’¸é¤¾çš„å±¤ID
                target_layers = self.distillation_layers
                
                # é è¨­å‡çµæ‰€æœ‰å±¤
                for name, param in self.model.named_parameters():
                    param.requires_grad = False
                
                # åªè§£å‡ç›®æ¨™å±¤çš„cv2.convåƒæ•¸
                unfrozen_count = 0
                unfrozen_names = []
                for name, param in self.model.named_parameters():
                    if "model." in name and any(f".{layer}." in name for layer in target_layers):
                        if ".cv2.conv" in name:  # ç²¾ç¢ºåŒ¹é…cv2çš„å·ç©å±¤åƒæ•¸
                            param.requires_grad = True
                            unfrozen_count += 1
                            unfrozen_names.append(name)
                
                # è¨ˆç®—å¯è¨“ç·´åƒæ•¸æ¯”ä¾‹
                total_params = sum(p.numel() for p in self.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                
                LOGGER.info(f"ç´”è’¸é¤¾æ¨¡å¼ï¼šåªå„ªåŒ–å±¤ {target_layers} ä¸­çš„ cv2.conv åƒæ•¸")
                LOGGER.info(f"è§£å‡äº† {unfrozen_count} å€‹åƒæ•¸çµ„ï¼Œå¯è¨“ç·´åƒæ•¸æ¯”ä¾‹: {trainable_params/total_params:.2%}")

                # è¨˜éŒ„å‡çµç‹€æ…‹çš„æ›´è©³ç´°è³‡è¨Š
                LOGGER.info("--------- åƒæ•¸å‡çµç‹€æ…‹ç¸½çµ ---------")
                LOGGER.info("ä»¥ä¸‹åƒæ•¸å°‡è¢«è¨“ç·´ (requires_grad=True):")
                for name in unfrozen_names:
                    LOGGER.info(f"  - {name}")
                
                # å‡çµæ‰€æœ‰BNå±¤ä¸¦è¨˜éŒ„
                bn_layer_names = []
                for name, m in self.model.named_modules():
                    if isinstance(m, torch.nn.BatchNorm2d):
                        m.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
                        m.track_running_stats = False  # åœæ­¢æ›´æ–°çµ±è¨ˆé‡
                        bn_layer_names.append(name)
                
                LOGGER.info(f"\nå·²å‡çµ {len(bn_layer_names)} å€‹ BN å±¤ï¼Œé€™äº›å±¤ä¸æœƒæ›´æ–°çµ±è¨ˆé‡:")
                # é¡¯ç¤ºéƒ¨åˆ†BNå±¤åç¨±ä½œç‚ºç¤ºä¾‹
                for i, name in enumerate(bn_layer_names):
                    if i < 10 or i >= len(bn_layer_names) - 5:  # é¡¯ç¤ºå‰10å€‹å’Œæœ€å¾Œ5å€‹
                        LOGGER.info(f"  - {name}")
                    elif i == 10:
                        LOGGER.info(f"  ... (çœç•¥ {len(bn_layer_names) - 15} å€‹ BN å±¤) ...")
                
                LOGGER.info("ç´”è’¸é¤¾æ¨¡å¼: æ‰€æœ‰BNå±¤å·²å‡çµï¼Œä¸å†æ›´æ–°çµ±è¨ˆé‡")
                LOGGER.info("------------------------------------")

    def distill_on_epoch_start(self, trainer):
        """æ¯å€‹ epoch é–‹å§‹æ™‚è¨»å†Šé‰¤å­"""
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            self.distill_loss_instance.register_hook()
            # æ·»åŠ é€™è¡Œä¾†æ¸¬è©¦é‰¤å­æ˜¯å¦æ­£ç¢ºè¨»å†Š
            for i, h in enumerate(self.distill_loss_instance.remove_handle):
                LOGGER.info(f"é‰¤å­ {i+1} å·²è¨»å†Š: {h}")

    def distill_on_epoch_end(self, trainer):
        """æ¯å€‹ epoch çµæŸæ™‚å–æ¶ˆé‰¤å­"""
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            self.distill_loss_instance.remove_handle_()
            LOGGER.debug(f"Removed distillation hooks at epoch {self.epoch} end")

    def distill_on_val_start(self, validator):
        """é©—è­‰é–‹å§‹æ™‚ç¢ºä¿é‰¤å­è¢«ç§»é™¤"""
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            # ç¢ºä¿åœ¨é©—è­‰éç¨‹ä¸­æ²’æœ‰é‰¤å­
            self.distill_loss_instance.remove_handle_()
            LOGGER.debug("Ensuring distillation hooks are removed for validation")

    def distill_on_val_end(self, validator):
        """é©—è­‰çµæŸå¾Œä¸éœ€è¦åšä»»ä½•äº‹ï¼Œå› ç‚ºæ¯å€‹ epoch é–‹å§‹æ™‚æœƒé‡æ–°è¨»å†Šé‰¤å­"""
        pass

    def distill_on_train_end(self, trainer):
        """è¨“ç·´çµæŸæ™‚æ¸…ç†è³‡æº"""
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            self.distill_loss_instance.remove_handle_()
            LOGGER.info("Cleaned up distillation resources at training end")

    def distill_teardown(self, trainer):
        """æœ€çµ‚æ¸…ç†ï¼Œç¢ºä¿é‰¤å­è¢«ç§»é™¤"""
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            try:
                self.distill_loss_instance.remove_handle_()
            except Exception as e:
                LOGGER.warning(f"Error removing hooks during teardown: {e}")
            self.distill_loss_instance = None
            LOGGER.debug("Final cleanup of distillation resources during teardown")

    def get_model(self, cfg=None, weights=None, verbose=True):
        """
        Get pose estimation model with specified configuration and weights.

        Args:
            cfg (str | Path | dict | None): Model configuration file path or dictionary.
            weights (str | Path | None): Path to the model weights file.
            verbose (bool): Whether to display model information.

        Returns:
            (PoseModel): Initialized pose estimation model.
        """
        model = PoseModel(cfg, ch=3, nc=self.data["nc"], data_kpt_shape=self.data["kpt_shape"], verbose=verbose)
        if weights:
            model.load(weights)

        return model
    
    def preprocess_batch(self, batch):
        """é è™•ç†æ‰¹æ¬¡æ•¸æ“šä¸¦æ·»åŠ è’¸é¤¾ç›¸é—œä¿¡æ¯"""
        batch = super().preprocess_batch(batch)
        
        # æ·»åŠ è’¸é¤¾ç›¸é—œä¿¡æ¯ï¼Œåƒ…åœ¨å•Ÿç”¨è’¸é¤¾æ™‚æ·»åŠ 
        if hasattr(self, 'distill_loss_instance') and self.distill_loss_instance is not None:
            batch["distill_instance"] = self.distill_loss_instance
            LOGGER.debug(f"Added distillation instance to batch for epoch {self.epoch}")

        # æ·»åŠ ç´”è’¸é¤¾æ¨™èªŒ
        if hasattr(self, 'pure_distill') and self.pure_distill:
            batch["pure_distill"] = True
        
        return batch

    def set_model_attributes(self):
        """Sets keypoints shape attribute of PoseModel."""
        super().set_model_attributes()
        self.model.kpt_shape = self.data["kpt_shape"]

    def get_validator(self):
        """Returns an instance of the PoseValidator class for validation."""
        self.loss_names = "box_loss", "pose_loss", "kobj_loss", "cls_loss", "dfl_loss", "d_loss"
        return yolo.pose.PoseValidator(
            self.test_loader,
            save_dir=self.save_dir,
            args=copy(self.args),
            _callbacks=self.callbacks
        )

    def plot_training_samples(self, batch, ni):
        """
        Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.

        Args:
            batch (dict): Dictionary containing batch data with the following keys:
                - img (torch.Tensor): Batch of images
                - keypoints (torch.Tensor): Keypoints coordinates for pose estimation
                - cls (torch.Tensor): Class labels
                - bboxes (torch.Tensor): Bounding box coordinates
                - im_file (list): List of image file paths
                - batch_idx (torch.Tensor): Batch indices for each instance
            ni (int): Current training iteration number used for filename

        The function saves the plotted batch as an image in the trainer's save directory with the filename
        'train_batch{ni}.jpg', where ni is the iteration number.
        """
        images = batch["img"]
        kpts = batch["keypoints"]
        cls = batch["cls"].squeeze(-1)
        bboxes = batch["bboxes"]
        paths = batch["im_file"]
        batch_idx = batch["batch_idx"]
        plot_images(
            images,
            batch_idx,
            cls,
            bboxes,
            kpts=kpts,
            paths=paths,
            fname=self.save_dir / f"train_batch{ni}.jpg",
            on_plot=self.on_plot,
        )

    def plot_metrics(self):
        """Plots training/val metrics."""
        plot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png
